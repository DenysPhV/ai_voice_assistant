# utils/ai_processor.py
import re
import os
import asyncio
import torch
from transformers import pipeline
# from dotenv import load_dotenv


# load_dotenv()
# TOKEN_LLAMA = os.getenv("TOKEN_LLAMA")

# –ú–∏ –±—ñ–ª—å—à–µ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ BitsAndBytesConfig –∞–±–æ AutoModelForCausalLM
model_name = "Qwen/Qwen2.5-1.5B-Instruct"
chat = None

def load_ai_model():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å Llama-3-8B-Instruct (Hugging Face).
    –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è GPU / CPU.
    """
    global chat
    
    if chat is not None:
        print("‚úÖ –ú–æ–¥–µ–ª—å Llama-3 –≤–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞.")
        return

    print(f"üöÄ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ {model_name}...")
    
    try:
        # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞ GPU, —è–∫—â–æ –º–æ–∂–ª–∏–≤–æ
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        chat = pipeline(
            "text-generation",
            model=model_name,
            # token=TOKEN_LLAMA,
            device=device,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            max_new_tokens=350,
            do_sample=True,
            temperature=0.5,
            top_p=0.9
        )
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –≥–æ—Ç–æ–≤–∞ (–Ω–∞ –ø—Ä–∏—Å—Ç—Ä–æ—ó: {device}).")
    
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {model_name}: {e}")
        print("–°–ø—Ä–æ–±–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞ CPU...")
        chat = pipeline(
            "text-generation",
            model=model_name,
            device="cpu",
            torch_dtype=torch.float32
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–ø—É—â–µ–Ω–∞ –Ω–∞ CPU.")


def llm_generate(prompt: str) -> str:
    """
    –í–∏–∫–ª–∏–∫–∞—î Qwen –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.
    Qwen –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î [INST], –∞–ª–µ –ø—Ä–∞—Ü—é—î —á—É–¥–æ–≤–æ
    –∑ —Å–∏—Å—Ç–µ–º–Ω–∏–º–∏ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è–º–∏.
    """
    if chat is None:
        return "–ü–æ–º–∏–ª–∫–∞: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞."
    
    full_prompt = (
        "You are a university assistant AI. "
        "Respond concisely, accurately, and in Ukrainian.\n\n"
        f"### –ó–∞–ø–∏—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞:\n{prompt}\n\n### –í—ñ–¥–ø–æ–≤—ñ–¥—å:"
    )
    # TinyLlama –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î [INST]
    # outputs = chat(
    #     prompt,
    #     max_new_tokens=300,
    #     temperature=0.6,
    #     top_p=0.9,
    #     do_sample=True,
    # )
    output = chat(full_prompt)[0]["generated_text"]
    if "### –í—ñ–¥–ø–æ–≤—ñ–¥—å:" in output:
        output = output.split("### –í—ñ–¥–ø–æ–≤—ñ–¥—å:")[-1].strip()

    # –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –ø–æ–≤—Ç–æ—Ä –ø—Ä–æ–º–ø—Ç–∞
    response = output

    # –ß–∞—Å—Ç–æ –º–æ–¥–µ–ª—å –¥–æ–¥–∞—î "–í—ñ–¥–ø–æ–≤—ñ–¥—å:" ‚Äî –ø—Ä–∏–±–∏—Ä–∞—î–º–æ
    if response.lower().startswith("–≤—ñ–¥–ø–æ–≤—ñ–¥—å"):
        response = response.split(":", 1)[-1].strip()

    # –ó–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –ø–æ–≤—Ç–æ—Ä–∏
    response = re.sub(r'\s+', ' ', response).strip()

    return response or "–í–∏–±–∞—á, –Ω–µ –∑–º—ñ–≥ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –∑–∞–ø–∏—Ç üòÖ" 


async def process_query(text: str) -> str:
    """
    –û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ –æ–±—Ä–æ–±–∫–∏ –∑–∞–ø–∏—Ç—É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞:
    - —Ä–æ–∑–∫–ª–∞–¥ –∑–∞–Ω—è—Ç—å
    - –≥–æ–¥–∏–Ω–∏ –ø—Ä–∏–π–æ–º—É
    - –∞–±–æ –∑–≤–∏—á–∞–π–Ω–∏–π –¥—ñ–∞–ª–æ–≥
    """
    # ‚ÄºÔ∏è –í–ê–ñ–õ–ò–í–û: –ú–∏ –ø–æ–≤–∏–Ω–Ω—ñ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –¢–£–¢, 
    # –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ async-—Ñ—É–Ω–∫—Ü—ñ—ó, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø–æ–º–∏–ª–æ–∫ –≤ 'lifespan'
    from .university_tools import get_schedule, get_office_hours

    if not text.strip():
        return "–í–∏–±–∞—á, —è –Ω—ñ—á–æ–≥–æ –Ω–µ –ø–æ—á—É–≤. –°–ø—Ä–æ–±—É–π —â–µ —Ä–∞–∑."
    
    text_lower = text.lower()
    # data_from_db = None
    
    # –†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É –ø—Ä–æ —Ä–æ–∑–∫–ª–∞–¥
    if "—Ä–æ–∑–∫–ª–∞–¥" in text_lower or "–ø–∞—Ä—É" in text_lower:
        match = re.search(r'([0-9]{2,3}\s?[A-Za-z–ê-–Ø–∞-—è–ú–º])', text, re.IGNORECASE)

        if not match:
            return "–ë—É–¥—å –ª–∞—Å–∫–∞, –≤–∫–∞–∂—ñ—Ç—å –Ω–æ–º–µ—Ä –≥—Ä—É–ø–∏. –ù–∞–ø—Ä–∏–∫–ª–∞–¥: '–†–æ–∑–∫–ª–∞–¥ –¥–ª—è 241–ú'."

        group = match.group(1).replace(" ", "")
        data = await get_schedule(group, "—Å—å–æ–≥–æ–¥–Ω—ñ")

        prompt = (
            f"–°—Ç—É–¥–µ–Ω—Ç –∑–∞–ø–∏—Ç—É—î —Ä–æ–∑–∫–ª–∞–¥ –¥–ª—è –≥—Ä—É–ø–∏ {group}. "
            f"–û—Å—å –¥–∞–Ω—ñ –∑ –±–∞–∑–∏: {data}. "
            "–°—Ñ–æ—Ä–º—É–π –∫–æ—Ä–æ—Ç–∫—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é."
        )
        return llm_generate(prompt)
        
    
    # –ì–æ–¥–∏–Ω–∏ –ø—Ä–∏–π–æ–º—É –≤–∏–∫–ª–∞–¥–∞—á–∞
    if "–ø—Ä–∏–π–æ–º—É" in text_lower or "–ø—Ä–∏–π–º–∞—î" in text_lower:
        match = re.search(r"(–ø—Ä–∏–π–æ–º—É|–ø—Ä–∏–π–º–∞—î)\s+([–ê-–Ø–∞-—è–Ü—ñ–á—ó–Ñ—î']+)", text_lower)

        if not match:
            return "–í–∫–∞–∂—ñ—Ç—å –ø—Ä—ñ–∑–≤–∏—â–µ –≤–∏–∫–ª–∞–¥–∞—á–∞, —â–æ–± —è –º—ñ–≥ –∑–Ω–∞–π—Ç–∏ –≥–æ–¥–∏–Ω–∏ –ø—Ä–∏–π–æ–º—É."

        professor = match.group(2)
        data = await get_office_hours(professor)

        prompt = (
            f"–°—Ç—É–¥–µ–Ω—Ç –∑–∞–ø–∏—Ç—É—î –≥–æ–¥–∏–Ω–∏ –ø—Ä–∏–π–æ–º—É –≤–∏–∫–ª–∞–¥–∞—á–∞ {professor}. "
            f"–û—Å—å –¥–∞–Ω—ñ –∑ –±–∞–∑–∏: {data}. "
            "–°—Ñ–æ—Ä–º—É–π –∫–æ—Ä–æ—Ç–∫—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é."
        )
        return llm_generate(prompt)

    # --- –ó–≤–∏—á–∞–π–Ω–∏–π —á–∞—Ç ---
    default_prompt = (
        f"–°—Ç—É–¥–µ–Ω—Ç –∑–∞–¥–∞—î –ø–∏—Ç–∞–Ω–Ω—è: '{text}'. "
        "–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —á—ñ—Ç–∫–æ —ñ –ø–æ —Å—É—Ç—ñ, —è–∫ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å—å–∫–∏–π –∞—Å–∏—Å—Ç–µ–Ω—Ç."
    )

    return llm_generate(default_prompt)
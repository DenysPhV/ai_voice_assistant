# utils/ai_processor.py
import re
import asyncio
import torch
from transformers import pipeline

# –ú–∏ –±—ñ–ª—å—à–µ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ BitsAndBytesConfig –∞–±–æ AutoModelForCausalLM
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
chat = None

def load_ai_model():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î TinyLlama. 
    –¶—è –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –º–∞–ª–∞, —â–æ–± –ø–æ–º—ñ—Å—Ç–∏—Ç–∏—Å—è —É VRAM –±–µ–∑ 
    —Å–∫–ª–∞–¥–Ω–æ—ó 4-–±—ñ—Ç–Ω–æ—ó –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó.
    """
    global chat
    
    if chat is not None:
        print("‚úÖ –ú–æ–¥–µ–ª—å TinyLlama –≤–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞.")
        return

    print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ {model_name}... –¶–µ –∑–∞–π–º–µ 1-2 —Ö–≤–∏–ª–∏–Ω–∏.")
    
    try:
        # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞ GPU, —è–∫—â–æ –º–æ–∂–ª–∏–≤–æ
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        chat = pipeline(
            "text-generation",
            model=model_name,
            device=device,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32
        )
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –≥–æ—Ç–æ–≤–∞ (–Ω–∞ –ø—Ä–∏—Å—Ç—Ä–æ—ó: {device}).")
    
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è TinyLlama: {e}")
        print("–°–ø—Ä–æ–±–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞ CPU...")
        # –Ø–∫—â–æ GPU –Ω–µ –≤–¥–∞–ª–æ—Å—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, —á–µ—Ä–µ–∑ —Ç—É —Å–∞–º—É –ø–æ–º–∏–ª–∫—É CUDA), 
        # –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –Ω–∞ CPU.
        chat = pipeline(
            "text-generation",
            model=model_name,
            device="cpu",
            torch_dtype=torch.float32
        )
        print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –≥–æ—Ç–æ–≤–∞ (–Ω–∞ –ø—Ä–∏—Å—Ç—Ä–æ—ó: cpu).")


def llm_generate(prompt: str) -> str:
    """
    –í–∏–∫–ª–∏–∫–∞—î TinyLlama, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Ñ–æ—Ä–º–∞—Ç [INST].
    """
    if chat is None:
        return "–ü–æ–º–∏–ª–∫–∞: –ú–æ–¥–µ–ª—å TinyLlama –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞."

    # TinyLlama –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î [INST]
    result = chat(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)[0]['generated_text']
    
    if '[/INST]' in result:
        result = result.split('[/INST]')[-1]

    result = result.strip()
    result = re.sub(r'\[\/?(INST|USER)\]', '', result).strip()
    result = re.sub(r'^[^:]+:\s*', '', result)
    result = re.sub(r'\s+', ' ', result).strip()

    if not result:
        return "–í–∏–±–∞—á, –Ω–µ –∑–º—ñ–≥ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –∑–∞–ø–∏—Ç üòÖ"
        
    return result

async def process_query(text: str) -> str:
    """
    –û–Ω–æ–≤–ª–µ–Ω–∏–π –ø—Ä–æ—Ü–µ—Å–æ—Ä, —è–∫–∏–π –º–æ–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏.
    (–¶–µ–π –∫–æ–¥ –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è —Ç–∞–∫–∏–º —Å–∞–º–∏–º, –∞–ª–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î [INST] –ø—Ä–æ–º–ø—Ç)
    """
    # ‚ÄºÔ∏è –í–ê–ñ–õ–ò–í–û: –ú–∏ –ø–æ–≤–∏–Ω–Ω—ñ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –¢–£–¢, 
    # –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ async-—Ñ—É–Ω–∫—Ü—ñ—ó, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø–æ–º–∏–ª–æ–∫ –≤ 'lifespan'
    from .university_tools import get_schedule, get_office_hours

    if not text.strip():
        return "–í–∏–±–∞—á, —è –Ω—ñ—á–æ–≥–æ –Ω–µ –ø–æ—á—É–≤. –°–ø—Ä–æ–±—É–π —â–µ —Ä–∞–∑."

    text_lower = text.lower()
    
    schedule_intent = re.search(r'—Ä–æ–∑–∫(–ª–∞–¥|–∞—Ç|–∞–¥|–æ–¥)', text_lower)
    
    if schedule_intent:
        group_match = re.search(r'([0-9]{2,3}\s?[A-Z–ú])', text, re.IGNORECASE) 
        if group_match:
            group_name = re.sub(r'\s', '', group_match.group(1))
            date = "—Å—å–æ–≥–æ–¥–Ω—ñ"
            
            # –í–∏–∫–ª–∏–∫–∞—î–º–æ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
            data_from_db = await get_schedule(group_name, date)
            
            # ‚ÄºÔ∏è –ü–û–í–ï–†–¢–ê–Ñ–ú–û–°–¨ –î–û [INST] –ü–†–û–ú–ü–¢–£ ‚ÄºÔ∏è
            prompt = f"[INST] –¢–∏ ‚Äî –∞—Å–∏—Å—Ç–µ–Ω—Ç —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –∑ —Ä—ñ—á–Ω–∏–º –¥–æ—á–≤—ñ–¥–æ–º. –ù–∞–¥–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –∑–∞–ø–∏—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –Ω–∞–¥–∞–Ω—ñ –¥–∞–Ω—ñ. –ó–∞–ø–∏—Ç: '{text}', –î–∞–Ω—ñ –∑ –±–∞–∑–∏: '{data_from_db}' [/INST]"
            response = llm_generate(prompt)
            return response
        else:
            return "–Ø –ø–æ—á—É–≤, —â–æ –≤–∏ —à—É–∫–∞—î—Ç–µ —Ä–æ–∑–∫–ª–∞–¥, –∞–ª–µ –Ω–µ –∑–º—ñ–≥ —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ –Ω–æ–º–µ—Ä –≥—Ä—É–ø–∏. –°–ø—Ä–æ–±—É–π—Ç–µ —Å–∫–∞–∑–∞—Ç–∏ —á—ñ—Ç–∫—ñ—à–µ, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥: '–†–æ–∑–∫–ª–∞–¥ –¥–ª—è 241–ú'."

    if "–≥–æ–¥–∏–Ω–∏ –ø—Ä–∏–π–æ–º—É" in text_lower or "–ø—Ä–∏–π–º–∞—î" in text_lower:
        professor_match = re.search(r'(–ø—Ä–∏–π–æ–º—É|–ø—Ä–∏–π–º–∞—î)\s+([–ê-–Ø–∞-—è–Ü—ñ–á—ó\']+)', text_lower)
        if professor_match:
            professor_name = professor_match.group(2)
            data_from_db = await get_office_hours(professor_name)
            
            prompt = f"[INST] –¢–∏ ‚Äî –∞—Å–∏—Å—Ç–µ–Ω—Ç —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—É. –ù–∞–¥–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –∑–∞–ø–∏—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –Ω–∞–¥–∞–Ω—ñ –¥–∞–Ω—ñ. –ó–∞–ø–∏—Ç: '{text}', –î–∞–Ω—ñ –∑ –±–∞–∑–∏: '{data_from_db}' [/INST]"
            response = llm_generate(prompt)
            return response
        else:
            return "–Ø –º–æ–∂—É –Ω–∞–¥–∞—Ç–∏ –≥–æ–¥–∏–Ω–∏ –ø—Ä–∏–π–æ–º—É, –∞–ª–µ, –±—É–¥—å –ª–∞—Å–∫–∞, –≤–∫–∞–∂—ñS—å –ø—Ä—ñ–∑–≤–∏—â–µ –≤–∏–∫–ª–∞–¥–∞—á–∞."

    # --- –ó–≤–∏—á–∞–π–Ω–∏–π —á–∞—Ç ---
    prompt = f"[INST] –¢–∏ ‚Äî —Ä–æ–∑–º–æ–≤–Ω–∏–π –∞—Å–∏—Å—Ç–µ–Ω—Ç. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ, —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. –ó–∞–ø–∏—Ç: {text} [/INST]"
    response = llm_generate(prompt)
    return response